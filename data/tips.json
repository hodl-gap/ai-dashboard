{
  "metadata": {
    "timestamp": "2026-01-08T16:17:26.575500",
    "total_articles": 8,
    "dedup_summary": {
      "total_input": 8,
      "unique_kept": 8,
      "duplicates_removed": 0,
      "url_duplicates": 0,
      "semantic_auto_duplicates": 0,
      "semantic_llm_confirmed": 0,
      "stored_to_db": 8
    },
    "cost": {
      "total_cost": "$0.0000",
      "input_tokens": 0,
      "output_tokens": 0,
      "llm_calls": 0
    }
  },
  "articles": [
    {
      "date": "2026-01-08",
      "source": "Hada News",
      "region": "Global",
      "category": "General",
      "layer": "Text_Llm",
      "contents": "K-뷰티 제품 문구의 기계번역 오역 사례 제시와 소비자 오해 리스크 분석. 도메인 특화 AI 번역 에이전트 설계, 현지화 전략 및 표현 보정 방법 소개.",
      "url": "https://news.hada.io/topic?id=25659",
      "title": "K-뷰티, 번역기 돌리면 떡핵합니다. 그래서 AI 번역 에이전트를 만들었습니다.️",
      "full_content": "<p>&quot;진정에 좋다&quot; → &quot;Good for calming&quot; (❌) 미국 Z세대는 이 문구를 보고 지갑을 열지 않습니다.</p>\n<p>그들이 반응하는 언어는 &quot;Game-changer for redness&quot; 혹은 &quot;Barrier repair with Artemisia&quot; 같은 '증명된 혜택'입니다 .</p>\n<p>K-뷰티의 미국 수출은 역대 최고치를",
      "source_type": "rss"
    },
    {
      "date": "2026-01-08",
      "source": "Hada News",
      "region": "Global",
      "category": "General",
      "layer": "Multimodal",
      "contents": "NIA의 2026년 12대 트렌드 핵심 요약: AI 신격전지(새 격전지), AI 인프라·인프라 권한 문제, 산업 현장 AI 적용, 6G 및 특수통신 영향, 공공 AI 보안 기술 강화 등 주요 이슈 정리. 정책·산업 준비 포인트와 권장 기술 방향 제시.",
      "url": "https://news.hada.io/topic?id=25657",
      "title": "NIA가 전달한 2026년 12대 AI·디지털 트렌드 [28p PDF]",
      "full_content": "<p>① AI의 새 격전지, AI 인프라 패권 경쟁 심화<br />\n② 스스로 일하는 AI에이전트, 협업과 자동화로 재편되는 미래<br />\n③ AI가 현실 세계로, 산업 현장에서 시작되는 피지컬 AI 혁신<br />\n④ 우주에서 지상까지 연결되는 6G와 위성통신의 융합<br />\n⑤ 똑똑해진 AI 공격에 맞서는 더 똑똑한 AI 보안 기술의 부상...</p>",
      "source_type": "rss"
    },
    {
      "date": "2026-01-08",
      "source": "Hada News",
      "region": "Global",
      "category": "General",
      "layer": "Text_Llm",
      "contents": "학생당 42센트로 AI 부정행위 탐지한 NYU 실험 결과와 방법 소개. Panos Ipeirotis가 제시한 저비용 AI 기반 검출 프로토콜, 실험 설계·데이터 수집 방법, 비용·효율성 분석 포함.",
      "url": "https://news.hada.io/topic?id=25656",
      "title": "학생당 42센트로 AI 부정행위 잡기: NYU 교수의 AI 구술시험 실험",
      "full_content": "<p>NYU 교수 AI 구술시험 실험</p>\n<p><strong>배경</strong></p>\n<ul>\n<li>AI 시대 전통 과제 평가 한계: 학생들이 AI로 과제 완벽히 작성하나 실제 이해 부족 드러남</li>\n<li>Panos Ipeirotis 교수 (NYU 스턴 경영대학원): AI로 AI 부정행위 대응 역발상 실험</li>\n</ul>\n<p><strong>실험 개요</strong></p>\n<ul>\n<li",
      "source_type": "rss"
    },
    {
      "date": "2026-01-07",
      "source": "Marktechpost",
      "region": "Global",
      "category": "General",
      "layer": "Text_Llm",
      "contents": "Transformer와 Mamba2 하이브리드 백본, vLLM을 통한 256k 토큰 문맥, SFT(긴 형식 추론 데이터)와 GRPO 기반 RL 훈련 레시피를 통한 설계 및 구현법 설명. 수학·코드 벤치마크에서 14B~47B 모델을 능가한 성능, 토큰 효율성 및 실무 적용 시 이점 소개.",
      "url": "https://www.marktechpost.com/2026/01/07/tii-abu-dhabi-released-falcon-h1r-7b-a-new-reasoning-model-outperforming-others-in-math-and-coding-with-only-7b-params-with-256k-context-window/",
      "title": "Falcon-H1R-7B: 256k 문맥과 GRPO 적용한 7B 추론 모델 출시",
      "full_content": "<p>Technology Innovation Institute (TII), Abu Dhabi, has released Falcon-H1R-7B, a 7B parameter reasoning specialized model that matches or exceeds many 14B to 47B reasoning models in math, code and general benchmarks, while staying compact and efficient. It builds on Falcon H1 7B Base and is available on Hugging Face under the Falcon-H1R collection.</p>\n\n\n\n<p>Falcon-H1R-7B is interesting because it combines 3 design choices in 1 system, a hybrid Transformer along with Mamba2 backbone, a very long context that reaches 256k tokens in standard vLLM deployments, and a training recipe that mixes supervised long form reasoning with reinforcement learning using GRPO.</p>\n\n\n\n<h3 class=\"wp-block-heading\" id=\"h-hybrid-transformer-plus-mamba2-architecture-with-long-context\"><strong>Hybrid Transformer plus Mamba2 architecture with long context</strong></h3>\n\n\n\n<p>Falcon-H1R-7B is a causal decoder only model with a hybrid architecture that combines Transformer layers and Mamba2 state space components. The Transformer blocks provide standard attention based reasoning, while the Mamba2 blocks give linear time sequence modeling and better memory scaling as context length grows. This design targets the 3 axes of reasoning efficiency that the team describes, speed, token efficiency and accuracy. </p>\n\n\n\n<p>The model runs with a default <code>--max-model-len</code> of <code>262144</code> when served through vLLM, which corresponds to a practical 256k token context window. This allows very long chain of thought traces, multi step tool use logs and large multi document prompts in a single pass. The hybrid backbone helps control memory use at these sequence lengths and improves throughput compared with a pure Transformer 7B baseline on the same hardware.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full is-resized\"><img alt=\"\" class=\"wp-image-77241\" height=\"1920\" src=\"https://www.marktechpost.com/wp-content/uploads/2026/01/1200x900-1-scaled.png\" style=\"width: 742px; height: auto;\" width=\"2560\" /></figure></div>\n\n\n<h3 class=\"wp-block-heading\" id=\"h-training-recipe-for-reasoning-tasks\"><strong>Training recipe for reasoning tasks</strong></h3>\n\n\n\n<p><strong>Falcon H1R 7B uses a 2 stage training pipeline:</strong></p>\n\n\n\n<p>In the<strong> first stage</strong>, the team runs cold start supervised fine tuning on top of Falcon-H1-7B Base. The SFT (supervised fine tuning) data mixes step by step long form reasoning traces in <strong>3 main domains</strong>, mathematics, coding and science, plus non reasoning domains such as chat, tool calling and safety. Difficulty aware filtering upweights harder problems and downweights trivial ones. Targets can reach up to 48k tokens, so the model sees long derivations and full solution paths during training.</p>\n\n\n\n<p>In the <strong>second stage</strong>, the SFT checkpoint is refined with GRPO, which is a group relative policy optimization method for reinforcement learning. Rewards are given when the generated reasoning chain is verifiably correct. For math problems, the system uses symbolic checks on the final answer. For code, it executes the generated program against unit tests. This RL stage pushes the model to keep useful intermediate steps while staying within a token budget. </p>\n\n\n\n<p>The result is a 7B model that is tuned specifically for chain of thought reasoning, rather than general chat.</p>\n\n\n\n<h3 class=\"wp-block-heading\" id=\"h-benchmarks-in-math-coding-and-general-reasoning\"><strong>Benchmarks in math, coding and general reasoning</strong></h3>\n\n\n\n<p>The Falcon-H1R-7B benchmark scores are grouped across math, code and agentic tasks, and general reasoning tasks.</p>\n\n\n\n<p>In the math group, Falcon-H1R-7B reaches an aggregate score of 73.96%, ahead of Apriel-1.5-15B at 69.32% and larger models like Qwen3-32B and Nemotron-H-47B. On individual benchmarks:</p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>AIME 24, 88.1%, higher than Apriel-1.5-15B at 86.2%</li>\n\n\n\n<li>AIME 25, 83.1%, higher than Apriel-1.5-15B at 80%</li>\n\n\n\n<li>HMMT 25, 64.9%, above all listed baselines</li>\n\n\n\n<li>AMO Bench, 36.3%, compared with 23.3% for DeepSeek-R1-0528 Qwen3-8B </li>\n</ul>\n\n\n\n<p>For code and agentic workloads, the model reaches 33.95% as a group score. On LiveCodeBench v6, Falcon-H1R-7B scores 68.6%, which is higher than Qwen3-32B and other baselines. It also scores 28.3% on the SciCode sub problem benchmark and 4.9% on Terminal Bench Hard, where it ranks second behind Apriel 1.5-15B but ahead of several 8B and 32B systems.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full is-resized\"><img alt=\"\" class=\"wp-image-77238\" height=\"554\" src=\"https://www.marktechpost.com/wp-content/uploads/2026/01/Screenshot-2026-01-07-at-3.38.11-AM-1.png\" style=\"width: 790px; height: auto;\" width=\"1470\" /><figcaption class=\"wp-element-caption\">https://huggingface.co/blog/tiiuae/falcon-h1r-7b</figcaption></figure></div>\n\n\n<p>On general reasoning, Falcon-H1R-7B achieves 49.48% as a group score. It records 61.3% on GPQA D, close to other 8B models, 72.1% on MMLU Pro, which is higher than all other 8B models in the above table, 11.1% on HLE and 53.4% on IFBench, where it is second only to Apriel 1.5 15B. </p>\n\n\n\n<p>The key takeaway is that a 7B model can sit in the same performance band as many 14B to 47B reasoning models, if the architecture and training pipeline are tuned for reasoning tasks.</p>\n\n\n\n<h3 class=\"wp-block-heading\" id=\"h-inference-throughput-and-test-time-scaling\"><strong>Inference throughput and test time scaling</strong></h3>\n\n\n\n<p>The team also benchmarked Falcon-H1R-7B on throughput and test time scaling under realistic batch settings.</p>\n\n\n\n<p>For a 512 token input and 32k token output, Falcon-H1R-7B reaches about 1,000 tokens per second per GPU at batch size 32 and about 1,500 tokens per second per GPU at batch size 64, nearly double the throughput of Qwen3-8B in the same configuration. For an 8k input and 16k output, Falcon-H1R-7B reaches around 1,800 tokens per second per GPU, while Qwen3-8B stays below 900. The hybrid Transformer along with Mamba architecture is a key factor in this scaling behavior, because it reduces the quadratic cost of attention for long sequences. </p>\n\n\n\n<p>Falcon-H1R-7B is also designed for test time scaling using Deep Think with confidence, known as DeepConf. The idea is to run many chains of thought in parallel, then use the model’s own next token confidence scores to filter noisy traces and keep only high quality candidates. </p>\n\n\n\n<p>On AIME 24 and AIME 25, Falcon-H1R-7B reaches 96.7% accuracy with fewer than 100 million generated tokens, which puts it on a favorable Pareto frontier of accuracy versus token cost compared with other 8B, 14B and 32B reasoning models. On the parser verifiable subset of AMO Bench, it reaches 35.9% accuracy with 217 million tokens, again ahead of the comparison models at similar or larger scale. </p>\n\n\n\n<h3 class=\"wp-block-heading\" id=\"h-key-takeaways\"><strong>Key Takeaways</strong></h3>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Falcon-H1R-7B is a 7B parameter reasoning model that uses a hybrid Transformer along with Mamba2 architecture and supports a 256k token context for long chain of thought prompts.</li>\n\n\n\n<li>The model is trained in 2 stages, supervised fine tuning on long reasoning traces in math, code and science up to 48k tokens, followed by GRPO based reinforcement learning with verifiable rewards for math and code.</li>\n\n\n\n<li>Falcon-H1R-7B achieves strong math performance, including about 88.1% on AIME 24, 83.1% on AIME 25 and a 73.96% aggregate math score, which is competitive with or better than larger 14B to 47B models.</li>\n\n\n\n<li>On coding and agentic tasks, Falcon-H1R-7B obtains 33.95% as a group score and 68.6% on LiveCodeBench v6, and it is also competitive on general reasoning benchmarks such as MMLU Pro and GPQA D.</li>\n\n\n\n<li>The hybrid design improves throughput, reaching around 1,000 to 1,800 tokens per second per GPU in the reported settings, and the model supports test time scaling through Deep Think with confidence to improve accuracy using multiple reasoning samples under a controlled token budget.</li>\n</ul>\n\n\n\n<hr class=\"wp-block-separator has-alpha-channel-opacity\" />\n\n\n\n<p>Check out the <a href=\"https://falcon-lm.github.io/blog/falcon-h1r-7b/\" rel=\"noreferrer noopener\" target=\"_blank\"><strong>Technical details</strong> </a>and <strong><a href=\"https://huggingface.co/collections/tiiuae/falcon-h1r\" rel=\"noreferrer noopener\" target=\"_blank\">MODEL WEIGHTS here</a></strong>. Also, feel free to follow us on <strong><a href=\"https://x.com/intent/follow?screen_name=marktechpost\" rel=\"noreferrer noopener\" target=\"_blank\">Twitter</a></strong> and don’t forget to join our <strong><a href=\"https://www.reddit.com/r/machinelearningnews/\" rel=\"noreferrer noopener\" target=\"_blank\">100k+ ML SubReddit</a></strong> and Subscribe to <strong><a href=\"https://www.aidevsignals.com/\" rel=\"noreferrer noopener\" target=\"_blank\">our Newsletter</a></strong>. Wait! are you on telegram? <strong><a href=\"https://t.me/machinelearningresearchnews\" rel=\"noreferrer noopener\" target=\"_blank\">now you can join us on telegram as well.</a></strong></p>\n\n\n\n<p>Check out our latest release of&nbsp;<a href=\"https://ai2025.dev/\" rel=\"noreferrer noopener\" target=\"_blank\"><strong>ai2025.dev</strong></a>, a 2025-focused analytics platform that turns model launches, benchmarks, and ecosystem activity into a structured dataset you can filter, compare, and export</p>\n<p>The post <a href=\"https://www.marktechpost.com/2026/01/07/tii-abu-dhabi-released-falcon-h1r-7b-a-new-reasoning-model-outperforming-others-in-math-and-coding-with-only-7b-params-with-256k-context-window/\">TII Abu-Dhabi Released Falcon H1R-7B: A New Reasoning Model Outperforming Others in Math and Coding with only 7B Params with 256k Context Window</a> appeared first on <a href=\"https://www.marktechpost.com\">MarkTechPost</a>.</p>",
      "source_type": "rss"
    },
    {
      "date": "2026-01-07",
      "source": "Marktechpost",
      "region": "Global",
      "category": "General",
      "layer": "Text_Llm",
      "contents": "Naive Softmax 구현에서 발생하는 오버플로/언더플로 사례와 문제 전파 메커니즘 설명. log-sum-exp 안정화 기법, PyTorch(torch) 예제 코드, 실패 재현 및 권장 구현법 제공.",
      "url": "https://www.marktechpost.com/2026/01/06/implementing-softmax-from-scratch-avoiding-the-numerical-stability-trap/",
      "title": "Softmax 수치 불안정 피하는 구현법과 log-sum-exp 활용 가이드",
      "full_content": "<p>In deep learning, classification models don’t just need to make predictions—they need to express confidence. That’s where the Softmax activation function comes in. Softmax takes the raw, unbounded scores produced by a neural network and transforms them into a well-defined probability distribution, making it possible to interpret each output as the likelihood of a specific class.&nbsp;</p>\n\n\n\n<p>This property makes Softmax a cornerstone of multi-class classification tasks, from image recognition to language modeling. In this article, we’ll build an intuitive understanding of how Softmax works and why its implementation details matter more than they first appear. Check out the <strong><a href=\"https://github.com/Marktechpost/AI-Tutorial-Codes-Included/blob/main/Data%20Science/Softmax.ipynb\" rel=\"noreferrer noopener\" target=\"_blank\">FULL CODES here</a></strong>.</p>\n\n\n\n<h2 class=\"wp-block-heading\" id=\"h-implementing-naive-softmax\"><strong>Implementing Naive Softmax</strong></h2>\n\n\n\n<div class=\"dm-code-snippet dark dm-normal-version default no-background-mobile\" style=\"background-color: #abb8c3;\"><div class=\"control-language\"><div class=\"dm-buttons\"><div class=\"dm-buttons-left\"><div class=\"dm-button-snippet red-button\"></div><div class=\"dm-button-snippet orange-button\"></div><div class=\"dm-button-snippet green-button\"></div></div><div class=\"dm-buttons-right\"><a id=\"dm-copy-raw-code\"><span class=\"dm-copy-text\">Copy Code</span><span class=\"dm-copy-confirmed\" style=\"display: none;\">Copied</span><span class=\"dm-error-message\" style=\"display: none;\">Use a different Browser</span></a></div></div><pre class=\" no-line-numbers\"><code class=\" no-wrap language-php\" id=\"dm-code-raw\">import torch\n\ndef softmax_naive(logits):\n    exp_logits = torch.exp(logits)\n    return exp_logits / exp_logits.sum(dim=1, keepdim=True)</code></pre></div></div>\n\n\n\n<p>This function implements the Softmax activation in its most straightforward form. It exponentiates each logit and normalizes it by the sum of all exponentiated values across classes, producing a probability distribution for each input sample.&nbsp;</p>\n\n\n\n<p>While this implementation is mathematically correct and easy to read, it is numerically unstable—large positive logits can cause overflow, and large negative logits can underflow to zero. As a result, this version should be avoided in real training pipelines. Check out the <strong><a href=\"https://github.com/Marktechpost/AI-Tutorial-Codes-Included/blob/main/Data%20Science/Softmax.ipynb\" rel=\"noreferrer noopener\" target=\"_blank\">FULL CODES here</a></strong>.</p>\n\n\n\n<h3 class=\"wp-block-heading\"><strong>Sample Logits and Target Labels</strong></h3>\n\n\n\n<p>This example defines a small batch with three samples and three classes to illustrate both normal and failure cases. The first and third samples contain reasonable logit values and behave as expected during Softmax computation. The second sample intentionally includes extreme values (1000 and -1000) to demonstrate numerical instability—this is where the naive Softmax implementation breaks down.&nbsp;</p>\n\n\n\n<p>The targets tensor specifies the correct class index for each sample and will be used to compute the classification loss and observe how instability propagates during backpropagation. Check out the <strong><a href=\"https://github.com/Marktechpost/AI-Tutorial-Codes-Included/blob/main/Data%20Science/Softmax.ipynb\" rel=\"noreferrer noopener\" target=\"_blank\">FULL CODES here</a></strong>.</p>\n\n\n\n<div class=\"dm-code-snippet dark dm-normal-version default no-background-mobile\" style=\"background-color: #abb8c3;\"><div class=\"control-language\"><div class=\"dm-buttons\"><div class=\"dm-buttons-left\"><div class=\"dm-button-snippet red-button\"></div><div class=\"dm-button-snippet orange-button\"></div><div class=\"dm-button-snippet green-button\"></div></div><div class=\"dm-buttons-right\"><a id=\"dm-copy-raw-code\"><span class=\"dm-copy-text\">Copy Code</span><span class=\"dm-copy-confirmed\" style=\"display: none;\">Copied</span><span class=\"dm-error-message\" style=\"display: none;\">Use a different Browser</span></a></div></div><pre class=\" no-line-numbers\"><code class=\" no-wrap language-php\" id=\"dm-code-raw\"># Batch of 3 samples, 3 classes\nlogits = torch.tensor([\n    [2.0, 1.0, 0.1],      \n    [1000.0, 1.0, -1000.0],  \n    [3.0, 2.0, 1.0]\n], requires_grad=True)\n\ntargets = torch.tensor([0, 2, 1])</code></pre></div></div>\n\n\n\n<h3 class=\"wp-block-heading\"><strong>Forward Pass: Softmax Output and the Failure Case</strong></h3>\n\n\n\n<p>During the forward pass, the naive Softmax function is applied to the logits to produce class probabilities. For normal logit values (first and third samples), the output is a valid probability distribution where values lie between 0 and 1 and sum to 1. </p>\n\n\n\n<p>However, the second sample clearly exposes the numerical issue: exponentiating 1000 overflows to <strong>infinity</strong>, while -1000 underflows to <strong>zero</strong>. This results in invalid operations during normalization, producing NaN values and zero probabilities. Once <strong>NaN </strong>appears at this stage, it contaminates all subsequent computations, making the model unusable for training. Check out the <strong><a href=\"https://github.com/Marktechpost/AI-Tutorial-Codes-Included/blob/main/Data%20Science/Softmax.ipynb\" rel=\"noreferrer noopener\" target=\"_blank\">FULL CODES here</a></strong>.</p>\n\n\n\n<div class=\"dm-code-snippet dark dm-normal-version default no-background-mobile\" style=\"background-color: #abb8c3;\"><div class=\"control-language\"><div class=\"dm-buttons\"><div class=\"dm-buttons-left\"><div class=\"dm-button-snippet red-button\"></div><div class=\"dm-button-snippet orange-button\"></div><div class=\"dm-button-snippet green-button\"></div></div><div class=\"dm-buttons-right\"><a id=\"dm-copy-raw-code\"><span class=\"dm-copy-text\">Copy Code</span><span class=\"dm-copy-confirmed\" style=\"display: none;\">Copied</span><span class=\"dm-error-message\" style=\"display: none;\">Use a different Browser</span></a></div></div><pre class=\" no-line-numbers\"><code class=\" no-wrap language-php\" id=\"dm-code-raw\"># Forward pass\nprobs = softmax_naive(logits)\n\nprint(\"Softmax probabilities:\")\nprint(probs)</code></pre></div></div>\n\n\n\n<h3 class=\"wp-block-heading\"><strong>Target Probabilities and Loss Breakdown</strong></h3>\n\n\n\n<p>Here, we extract the predicted probability corresponding to the true class for each sample. While the first and third samples return valid probabilities, the second sample’s target probability is 0.0, caused by numerical underflow in the Softmax computation. When the loss is calculated using <strong>-log(p)</strong>, taking the logarithm of 0.0 results in <strong>+∞</strong>. </p>\n\n\n\n<p>This makes the overall loss infinite, which is a critical failure during training. Once the loss becomes infinite, gradient computation becomes unstable, leading to <strong>NaNs </strong>during backpropagation and effectively halting learning. Check out the <strong><a href=\"https://github.com/Marktechpost/AI-Tutorial-Codes-Included/blob/main/Data%20Science/Softmax.ipynb\" rel=\"noreferrer noopener\" target=\"_blank\">FULL CODES here</a></strong>.</p>\n\n\n\n<div class=\"dm-code-snippet dark dm-normal-version default no-background-mobile\" style=\"background-color: #abb8c3;\"><div class=\"control-language\"><div class=\"dm-buttons\"><div class=\"dm-buttons-left\"><div class=\"dm-button-snippet red-button\"></div><div class=\"dm-button-snippet orange-button\"></div><div class=\"dm-button-snippet green-button\"></div></div><div class=\"dm-buttons-right\"><a id=\"dm-copy-raw-code\"><span class=\"dm-copy-text\">Copy Code</span><span class=\"dm-copy-confirmed\" style=\"display: none;\">Copied</span><span class=\"dm-error-message\" style=\"display: none;\">Use a different Browser</span></a></div></div><pre class=\" no-line-numbers\"><code class=\" no-wrap language-php\" id=\"dm-code-raw\"># Extract target probabilities\ntarget_probs = probs[torch.arange(len(targets)), targets]\n\nprint(\"\\nTarget probabilities:\")\nprint(target_probs)\n\n# Compute loss\nloss = -torch.log(target_probs).mean()\nprint(\"\\nLoss:\", loss)</code></pre></div></div>\n\n\n\n<h3 class=\"wp-block-heading\"><strong>Backpropagation: Gradient Corruption</strong></h3>\n\n\n\n<p>When backpropagation is triggered, the impact of the infinite loss becomes immediately visible. The gradients for the first and third samples remain finite because their Softmax outputs were well-behaved. However, the second sample produces NaN gradients across all classes due to the log(0) operation in the loss.&nbsp;</p>\n\n\n\n<p>These NaNs propagate backward through the network, contaminating weight updates and effectively breaking training. This is why numerical instability at the Softmax–loss boundary is so dangerous—once NaNs appear, recovery is nearly impossible without restarting training. Check out the <strong><a href=\"https://github.com/Marktechpost/AI-Tutorial-Codes-Included/blob/main/Data%20Science/Softmax.ipynb\" rel=\"noreferrer noopener\" target=\"_blank\">FULL CODES here</a></strong>.</p>\n\n\n\n<div class=\"dm-code-snippet dark dm-normal-version default no-background-mobile\" style=\"background-color: #abb8c3;\"><div class=\"control-language\"><div class=\"dm-buttons\"><div class=\"dm-buttons-left\"><div class=\"dm-button-snippet red-button\"></div><div class=\"dm-button-snippet orange-button\"></div><div class=\"dm-button-snippet green-button\"></div></div><div class=\"dm-buttons-right\"><a id=\"dm-copy-raw-code\"><span class=\"dm-copy-text\">Copy Code</span><span class=\"dm-copy-confirmed\" style=\"display: none;\">Copied</span><span class=\"dm-error-message\" style=\"display: none;\">Use a different Browser</span></a></div></div><pre class=\" no-line-numbers\"><code class=\" no-wrap language-php\" id=\"dm-code-raw\">loss.backward()\n\nprint(\"\\nGradients:\")\nprint(logits.grad)</code></pre></div></div>\n\n\n\n<h3 class=\"wp-block-heading\"><strong>Numerical Instability and Its Consequences</strong></h3>\n\n\n\n<p>Separating Softmax and cross-entropy creates a serious numerical stability risk due to exponential overflow and underflow. Large logits can push probabilities to infinity or zero, causing log(0) and leading to NaN gradients that quickly corrupt training. At production scale, this is not a rare edge case but a certainty—without stable, fused implementations, large multi-GPU training runs would fail unpredictably.&nbsp;</p>\n\n\n\n<p>The core numerical problem comes from the fact that computers cannot represent infinitely large or infinitely small numbers. Floating-point formats like FP32 have strict limits on how big or small a value can be stored. When Softmax computes exp(x), large positive values grow so fast that they exceed the maximum representable number and turn into infinity, while large negative values shrink so much that they become zero. Once a value becomes infinity or zero, subsequent operations like division or logarithms break down and produce invalid results. Check out the <strong><a href=\"https://github.com/Marktechpost/AI-Tutorial-Codes-Included/blob/main/Data%20Science/Softmax.ipynb\" rel=\"noreferrer noopener\" target=\"_blank\">FULL CODES here</a></strong>.</p>\n\n\n\n<figure class=\"wp-block-image size-full is-resized\"><img alt=\"\" class=\"wp-image-77232\" height=\"729\" src=\"https://www.marktechpost.com/wp-content/uploads/2026/01/image-3.png\" style=\"width: 511px; height: auto;\" width=\"581\" /></figure>\n\n\n\n<h2 class=\"wp-block-heading\"><strong>Implementing Stable Cross-Entropy Loss Using LogSumExp</strong></h2>\n\n\n\n<p>This implementation computes cross-entropy loss directly from raw logits without explicitly calculating Softmax probabilities. To maintain numerical stability, the logits are first shifted by subtracting the maximum value per sample, ensuring exponentials stay within a safe range.&nbsp;</p>\n\n\n\n<p>The LogSumExp trick is then used to compute the normalization term, after which the original (unshifted) target logit is subtracted to obtain the correct loss. This approach avoids overflow, underflow, and NaN gradients, and mirrors how cross-entropy is implemented in production-grade deep learning frameworks. Check out the <strong><a href=\"https://github.com/Marktechpost/AI-Tutorial-Codes-Included/blob/main/Data%20Science/Softmax.ipynb\" rel=\"noreferrer noopener\" target=\"_blank\">FULL CODES here</a></strong>.</p>\n\n\n\n<div class=\"dm-code-snippet dark dm-normal-version default no-background-mobile\" style=\"background-color: #abb8c3;\"><div class=\"control-language\"><div class=\"dm-buttons\"><div class=\"dm-buttons-left\"><div class=\"dm-button-snippet red-button\"></div><div class=\"dm-button-snippet orange-button\"></div><div class=\"dm-button-snippet green-button\"></div></div><div class=\"dm-buttons-right\"><a id=\"dm-copy-raw-code\"><span class=\"dm-copy-text\">Copy Code</span><span class=\"dm-copy-confirmed\" style=\"display: none;\">Copied</span><span class=\"dm-error-message\" style=\"display: none;\">Use a different Browser</span></a></div></div><pre class=\" no-line-numbers\"><code class=\" no-wrap language-php\" id=\"dm-code-raw\">def stable_cross_entropy(logits, targets):\n\n    # Find max logit per sample\n    max_logits, _ = torch.max(logits, dim=1, keepdim=True)\n\n    # Shift logits for numerical stability\n    shifted_logits = logits - max_logits\n\n    # Compute LogSumExp\n    log_sum_exp = torch.log(torch.sum(torch.exp(shifted_logits), dim=1)) + max_logits.squeeze(1)\n\n    # Compute loss using ORIGINAL logits\n    loss = log_sum_exp - logits[torch.arange(len(targets)), targets]\n\n    return loss.mean()</code></pre></div></div>\n\n\n\n<h3 class=\"wp-block-heading\"><strong>Stable Forward and Backward Pass</strong></h3>\n\n\n\n<p>Running the stable cross-entropy implementation on the same extreme logits produces a finite loss and well-defined gradients. Even though one sample contains very large values (1000 and -1000), the LogSumExp formulation keeps all intermediate computations in a safe numerical range. As a result, backpropagation completes successfully without producing NaNs, and each class receives a meaningful gradient signal.&nbsp;</p>\n\n\n\n<p>This confirms that the instability seen earlier was not caused by the data itself, but by the naive separation of Softmax and cross-entropy—an issue fully resolved by using a numerically stable, fused loss formulation. Check out the <strong><a href=\"https://github.com/Marktechpost/AI-Tutorial-Codes-Included/blob/main/Data%20Science/Softmax.ipynb\" rel=\"noreferrer noopener\" target=\"_blank\">FULL CODES here</a></strong>.</p>\n\n\n\n<div class=\"dm-code-snippet dark dm-normal-version default no-background-mobile\" style=\"background-color: #abb8c3;\"><div class=\"control-language\"><div class=\"dm-buttons\"><div class=\"dm-buttons-left\"><div class=\"dm-button-snippet red-button\"></div><div class=\"dm-button-snippet orange-button\"></div><div class=\"dm-button-snippet green-button\"></div></div><div class=\"dm-buttons-right\"><a id=\"dm-copy-raw-code\"><span class=\"dm-copy-text\">Copy Code</span><span class=\"dm-copy-confirmed\" style=\"display: none;\">Copied</span><span class=\"dm-error-message\" style=\"display: none;\">Use a different Browser</span></a></div></div><pre class=\" no-line-numbers\"><code class=\" no-wrap language-php\" id=\"dm-code-raw\">logits = torch.tensor([\n    [2.0, 1.0, 0.1],\n    [1000.0, 1.0, -1000.0],\n    [3.0, 2.0, 1.0]\n], requires_grad=True)\n\ntargets = torch.tensor([0, 2, 1])\n\nloss = stable_cross_entropy(logits, targets)\nprint(\"Stable loss:\", loss)\n\nloss.backward()\nprint(\"\\nGradients:\")\nprint(logits.grad)</code></pre></div></div>\n\n\n\n<figure class=\"wp-block-image size-full is-resized\"><img alt=\"\" class=\"wp-image-77233\" height=\"757\" src=\"https://www.marktechpost.com/wp-content/uploads/2026/01/image-4.png\" style=\"width: 567px; height: auto;\" width=\"783\" /></figure>\n\n\n\n<h2 class=\"wp-block-heading\"><strong>Conclusion</strong></h2>\n\n\n\n<p>In practice, the gap between mathematical formulas and real-world code is where many training failures originate. While Softmax and cross-entropy are mathematically well-defined, their naive implementation ignores the finite precision limits of IEEE 754 hardware, making underflow and overflow inevitable.&nbsp;</p>\n\n\n\n<p>The key fix is simple but critical: shift logits before exponentiation and operate in the log domain whenever possible. Most importantly, training rarely requires explicit probabilities—stable log-probabilities are sufficient and far safer. When a loss suddenly turns into NaN in production, it’s often a signal that Softmax is being computed manually somewhere it shouldn’t be.</p>\n\n\n\n<hr class=\"wp-block-separator has-alpha-channel-opacity\" />\n\n\n\n<p>Check out the <strong><a href=\"https://github.com/Marktechpost/AI-Tutorial-Codes-Included/blob/main/Data%20Science/Softmax.ipynb\" rel=\"noreferrer noopener\" target=\"_blank\">FULL CODES here</a></strong>. Also, feel free to follow us on <strong><a href=\"https://x.com/intent/follow?screen_name=marktechpost\" rel=\"noreferrer noopener\" target=\"_blank\">Twitter</a></strong> and don’t forget to join our <strong><a href=\"https://www.reddit.com/r/machinelearningnews/\" rel=\"noreferrer noopener\" target=\"_blank\">100k+ ML SubReddit</a></strong> and Subscribe to <strong><a href=\"https://www.aidevsignals.com/\" rel=\"noreferrer noopener\" target=\"_blank\">our Newsletter</a></strong>. Wait! are you on telegram? <strong><a href=\"https://t.me/machinelearningresearchnews\" rel=\"noreferrer noopener\" target=\"_blank\">now you can join us on telegram as well.</a></strong></p>\n\n\n\n<p>Check out our latest release of&nbsp;<a href=\"https://ai2025.dev/\" rel=\"noreferrer noopener\" target=\"_blank\"><strong>ai2025.dev</strong></a>, a 2025-focused analytics platform that turns model launches, benchmarks, and ecosystem activity into a structured dataset you can filter, compare, and export</p>\n<p>The post <a href=\"https://www.marktechpost.com/2026/01/06/implementing-softmax-from-scratch-avoiding-the-numerical-stability-trap/\">Implementing Softmax From Scratch: Avoiding the Numerical Stability Trap</a> appeared first on <a href=\"https://www.marktechpost.com\">MarkTechPost</a>.</p>",
      "source_type": "rss"
    },
    {
      "date": "2026-01-07",
      "source": "Marktechpost",
      "region": "Global",
      "category": "Agents",
      "layer": "Audio",
      "contents": "FastConformer 인코더(캐시 인식)와 RNNT 디코더 기반의 600M Nemotron Speech ASR 설계와 캐시 기반 스트리밍 처리 방식 설명. 80ms~1.12s 청크 설정별 WER(예: 0.16s에서 약 7.84%), H100 등 GPU에서의 처리량·동시성 이점 및 실시간 에이전트 적용 포인트 소개.",
      "url": "https://www.marktechpost.com/2026/01/06/nvidia-ai-released-nemotron-speech-asr-a-new-open-source-transcription-model-designed-from-the-ground-up-for-low-latency-use-cases-like-voice-agents/",
      "title": "Nemotron Speech ASR: 저지연 스트리밍용 600M 공개 ASR 모델 공개",
      "full_content": "<p>NVIDIA has just released its new streaming English transcription model (Nemotron Speech ASR) built specifically for low latency voice agents and live captioning. The checkpoint <code>nvidia/nemotron-speech-streaming-en-0.6b</code> on Hugging Face combines a cache aware FastConformer encoder with an RNNT decoder, and is tuned for both streaming and batch workloads on modern NVIDIA GPUs. </p>\n\n\n\n<h3 class=\"wp-block-heading\" id=\"h-model-design-architecture-and-input-assumptions\"><strong>Model design, architecture and input assumptions</strong></h3>\n\n\n\n<p>Nemotron Speech ASR (Automatic Speech Recognition) is a 600M parameter model based on a cache aware FastConformer encoder with 24 layers and an RNNT decoder. The encoder uses aggressive 8x convolutional downsampling to reduce the number of time steps, which directly lowers compute and memory costs for streaming workloads. The model consumes 16 kHz mono audio and requires at least 80 ms of input audio per chunk. </p>\n\n\n\n<p>Runtime latency is controlled through configurable context sizes. The model exposes 4 standard chunk configurations, corresponding to about 80 ms, 160 ms, 560 ms and 1.12 s of audio. These modes are driven by the <code>att_context_size</code> parameter, which sets left and right attention context in multiples of 80 ms frames, and can be changed at inference time without retraining. </p>\n\n\n\n<h3 class=\"wp-block-heading\" id=\"h-cache-aware-streaming-not-buffered-sliding-windows\"><strong>Cache aware streaming, not buffered sliding windows</strong></h3>\n\n\n\n<p>Traditional &#8216;streaming ASR&#8217; often uses overlapping windows. Each incoming window reprocesses part of the previous audio to maintain context, which wastes compute and causes latency to drift upward as concurrency increases.</p>\n\n\n\n<p>Nemotron Speech ASR instead keeps a cache of encoder states for all self attention and convolution layers. Each new chunk is processed once, with the model reusing cached activations rather than recomputing overlapping context. <strong>This gives:</strong></p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Non overlapping frame processing, so work scales linearly with audio length</li>\n\n\n\n<li>Predictable memory growth, because cache size grows with sequence length rather than concurrency related duplication</li>\n\n\n\n<li>Stable latency under load, which is critical for turn taking and interruption in voice agents</li>\n</ul>\n\n\n\n<h3 class=\"wp-block-heading\" id=\"h-accuracy-vs-latency-wer-under-streaming-constraints\"><strong>Accuracy vs latency: WER under streaming constraints</strong></h3>\n\n\n\n<p>Nemotron Speech ASR is evaluated on the Hugging Face OpenASR leaderboard datasets, including AMI, Earnings22, Gigaspeech and LibriSpeech. Accuracy is reported as word error rate (WER) for different chunk sizes. </p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full is-resized\"><img alt=\"\" class=\"wp-image-77228\" height=\"1563\" src=\"https://www.marktechpost.com/wp-content/uploads/2026/01/blog-banner23-11.png\" style=\"width: 692px; height: auto;\" width=\"2188\" /></figure></div>\n\n\n<p><strong>For an average across these benchmarks, the model achieves:</strong></p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>About 7.84 percent WER at 0.16 s chunk size</li>\n\n\n\n<li>About 7.22 percent WER at 0.56 s chunk size</li>\n\n\n\n<li>About 7.16 percent WER at 1.12 s chunk size </li>\n</ul>\n\n\n\n<p>This illustrates the latency accuracy tradeoff. Larger chunks give more phonetic context and slightly lower WER, but even the 0.16 s mode keeps WER under 8 percent while remaining usable for real time agents. Developers can choose the operating point at inference time depending on application needs, for example 160 ms for aggressive voice agents, or 560 ms for transcription centric workflows. </p>\n\n\n\n<h3 class=\"wp-block-heading\" id=\"h-throughput-and-concurrency-on-modern-gpus\"><strong>Throughput and concurrency on modern GPUs</strong></h3>\n\n\n\n<p>The cache aware design has measurable impact on concurrency. On an NVIDIA H100 GPU, Nemotron Speech ASR supports about 560 concurrent streams at a 320 ms chunk size, roughly 3x the concurrency of a baseline streaming system at the same latency target. RTX A5000 and DGX B200 benchmarks show similar throughput gains, with more than 5x concurrency on A5000 and up to 2x on B200 across typical latency settings.</p>\n\n\n\n<p>Equally important, latency remains stable as concurrency increases. In Modal’s tests with 127 concurrent WebSocket clients at 560 ms mode, the system maintained a median end to end delay around 182 ms without drift, which is essential for agents that must stay synchronized with live speech over multi minute sessions.</p>\n\n\n\n<h3 class=\"wp-block-heading\" id=\"h-training-data-and-ecosystem-integration\"><strong>Training data and ecosystem integration</strong></h3>\n\n\n\n<p>Nemotron Speech ASR is trained mainly on the English portion of NVIDIA’s Granary dataset along with a large mixture of public speech corpora, for a total of about 285k hours of audio. Datasets include YouTube Commons, YODAS2, Mosel, LibriLight, Fisher, Switchboard, WSJ, VCTK, VoxPopuli and multiple Mozilla Common Voice releases. Labels combine human and ASR generated transcripts.</p>\n\n\n\n<h3 class=\"wp-block-heading\" id=\"h-key-takeaways\"><strong>Key Takeaways</strong></h3>\n\n\n\n<ol class=\"wp-block-list\">\n<li>Nemotron Speech ASR is a 0.6B parameter English streaming model that uses a cache aware FastConformer encoder with an RNNT decoder and operates on 16 kHz mono audio with at least 80 ms input chunks.</li>\n\n\n\n<li>The model exposes 4 inference time chunk configurations, about 80 ms, 160 ms, 560 ms and 1.12 s, which let engineers trade latency for accuracy without retraining while keeping WER around 7.2 percent to 7.8 percent on standard ASR benchmarks.</li>\n\n\n\n<li>Cache aware streaming removes overlapping window recomputation so each audio frame is encoded once, which yields about 3 times higher concurrent streams on H100, more than 5 times on RTX A5000 and up to 2 times on DGX B200 compared to a buffered streaming baseline at similar latency.</li>\n\n\n\n<li>In an end to end voice agent with Nemotron Speech ASR, Nemotron 3 Nano 30B and Magpie TTS, measured median time to final transcription is about 24 ms and server side voice to voice latency on RTX 5090 is around 500 ms, which makes ASR a small fraction of the total latency budget.</li>\n\n\n\n<li>Nemotron Speech ASR is released as a NeMo checkpoint under the NVIDIA Permissive Open Model License with open weights and training details, so teams can self host, fine tune and profile the full stack for low latency voice agents and speech applications.</li>\n</ol>\n\n\n\n<hr class=\"wp-block-separator has-alpha-channel-opacity\" />\n\n\n\n<p>Check out the <strong><a href=\"https://huggingface.co/nvidia/nemotron-speech-streaming-en-0.6b\" rel=\"noreferrer noopener\" target=\"_blank\">MODEL WEIGHTS here</a></strong>. Also, feel free to follow us on <strong><a href=\"https://x.com/intent/follow?screen_name=marktechpost\" rel=\"noreferrer noopener\" target=\"_blank\">Twitter</a></strong> and don’t forget to join our <strong><a href=\"https://www.reddit.com/r/machinelearningnews/\" rel=\"noreferrer noopener\" target=\"_blank\">100k+ ML SubReddit</a></strong> and Subscribe to <strong><a href=\"https://www.aidevsignals.com/\" rel=\"noreferrer noopener\" target=\"_blank\">our Newsletter</a></strong>. Wait! are you on telegram? <strong><a href=\"https://t.me/machinelearningresearchnews\" rel=\"noreferrer noopener\" target=\"_blank\">now you can join us on telegram as well.</a></strong></p>\n\n\n\n<p>Check out our latest release of&nbsp;<a href=\"https://ai2025.dev/\" rel=\"noreferrer noopener\" target=\"_blank\"><strong>ai2025.dev</strong></a>, a 2025-focused analytics platform that turns model launches, benchmarks, and ecosystem activity into a structured dataset you can filter, compare, and export</p>\n<p>The post <a href=\"https://www.marktechpost.com/2026/01/06/nvidia-ai-released-nemotron-speech-asr-a-new-open-source-transcription-model-designed-from-the-ground-up-for-low-latency-use-cases-like-voice-agents/\">NVIDIA AI Released Nemotron Speech ASR: A New Open Source Transcription Model Designed from the Ground Up for Low-Latency Use Cases like Voice Agents</a> appeared first on <a href=\"https://www.marktechpost.com\">MarkTechPost</a>.</p>",
      "source_type": "rss"
    },
    {
      "date": "2026-01-07",
      "source": "@Sumanth_077",
      "region": "Global",
      "category": "Other",
      "layer": "B2B Applications",
      "contents": "Add persistent long-term memory to Claude Code, Gemini CLI, and other coding agents with a single line of code to run locally. This enhancement allows AI coding assistants to retain context and improve performance across multiple sessions without external dependencies.",
      "url": "https://x.com/Sumanth_077/status/2008898618959540589",
      "title": "You can now make Claude Code, Gemini CLI, and other coding agents 10x more powerful by giving them l...",
      "source_type": "twitter"
    },
    {
      "date": "2026-01-07",
      "source": "@Sumanth_077",
      "region": "Global",
      "category": "Other",
      "layer": "B2B Applications",
      "contents": "Use Byterover to manage AI coding agent context like Git version control—push local context to shared workspaces and pull it across Cursor, Windsurf, and Claude Code. Teams can maintain a single source of truth for AI-assisted development across different editors.",
      "url": "https://x.com/Sumanth_077/status/2008898647304630710",
      "title": "Byterover works exactly like Git. You push your local context to a remote workspace, and your teamma...",
      "source_type": "twitter"
    }
  ]
}